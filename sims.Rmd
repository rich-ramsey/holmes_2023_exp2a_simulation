---
title: "sims"
author: "Rich"
date: "2024-01-25"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file simulates data and runs power analyses based on the modelling of
Exp2a from Holmes et al., 2023.

This is a little bit redundant in some ways, as with simple designs, G*Power
could do a quick and dirty job, if you plug in a standardised effect sizes between
two conditions, for example. On that note, Cohen's dz is over 1 for key comparisons
in Holmes et al., 2023, which means N=12 would give you > 90% power.

But that kind of quick and dirty approach ignores the multi-level structure to the
data and varying effects by participant, which is the benefit of the multi-level
modelling approach taken here.


Most of the below is based on two sources:

1) The {faux} package: https://debruine.github.io/faux/

2) Solomon Kurz's blog post: https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/

## load the libraries that we will be using ## 

## install ##

you may only need the additional faux package here

```{r}
# install.packages(c("faux"))
```

if not, then run the next chunk, which installs them all.

```{r install-pkg}
# install.packages("remotes")
# remotes::install_github("stan-dev/cmdstanr")
# 
# install.packages("devtools")
# devtools::install_github("jmgirard/standist")
# 
# install.packages(c("tidyverse", "RColorBrewer", "patchwork", "brms",
#                    "tidybayes", "bayesplot", "patchwork", "future", "faux"))
```

take a snapshot of loaded packages and update the lock.file using renv

```{r snapshot-renv}
# take a snapshot and update the lock.file
# renv::snapshot() # this is only necessary when new packages or installed or packages are updated.
```

## load ##

```{r load-pkg}
pkg <- c("cmdstanr", "standist", "tidyverse", "RColorBrewer", "patchwork", 
         "brms", "tidybayes", "bayesplot", "future", "parallel", "modelr",
         "faux")

lapply(pkg, library, character.only = TRUE)
```

## settings ##

```{r set-options}
options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

supportsMulticore()

detectCores()
```

## plot settings ##

```{r}
## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

theme settings for ggplot

```{r, eval = F}
theme_set(
  theme_bw() +
    theme(text = element_text(size = 18), 
          title = element_text(size = 18),
          legend.position = "bottom")
)
```

## section 1 - read in prior models and data ##

this is just useful as a reminder of key values and structures

data

```{r}
data <- read_csv("data/data.csv")
head(data)
```

model fit

```{r}
b1 <- readRDS("models/b1.rds")
summary(b1)
```

model fit of simulated data

```{r}
# based on model b1
# fit <- readRDS("models/sims/fit.rds") 
# summary(fit)
```

## section 2 - simulate some data ##

## based on the count dv and b1 model ##

define some values

```{r}
# define parameters
# specify some features of the design
subj_n = 12  # number of subjects
rep_n = 8 # number of trial repeats 

# set fixed effects
b0 = 14.99    # intercept (no stimulation)
b1 = -1.71    # fixed effect of left stim < no stim
b2 = -0.16    # fixed effect of right stim < no stim

# set varying effects (by subject)
u0s_sd = 1.14   # varying intercept SD 
u1s_sd = 1.39   # varying b1 slope SD 
u2s_sd = 0.40   # varying b2 slope SD

# set correlations between varying effects
# cors between mean average effects first
r01s = -0.52   # correlation between varying effects 0 and 1 
r02s = -0.11   # correlation between varying effects 0 and 2 
r12s = 0.34  # correlation between varying effects 1 and 2 

# sigma
sigma_sd = 1.26 # error SD
```

make a correlation matrix 

```{r}
cors = c(r01s, r02s,
         r12s)
```

setup the data structure

```{r}
# make it reproducible
set.seed(1)

# set up data structure
d1 <- add_random(subj = subj_n, rep = rep_n) %>%
  # add and recode categorical variables
  add_within("subj", condition = c("none", "left", "right")) %>%
  add_contrast("condition", "treatment", add_cols = TRUE, 
               colnames = c("left", "right")) %>%
  # add random effects 
  add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd,
            .cors = cors) %>%
  add_ranef(sigma = sigma_sd) %>%
  # calculate dv
  mutate(count = b0 + u0s + (b1 + u1s) * left + (b2 + u2s) * right + sigma)  

head(d1)
str(d1)
summary(d1)

# save initial data
write_csv(d1, "data/sims/d1.csv") # 
```

data.check

```{r}
data.check <- d1 %>% 
  distinct(rep, condition)
data.check
```

density plot 

```{r}
p2.1 <- ggplot(d1, aes(x=count, fill=condition)) +
   geom_density(alpha = 0.3, colour = "darkgrey") +
   scale_fill_brewer(palette = "Dark2")+
   theme(panel.grid = element_blank()) +
   ggtitle("count by stimulation condition")
p2.1

ggsave ("figures/sims/d1_count_density.jpeg")
```

create some group average summary data

per pid

```{r}
summary_pid <- d1 %>% 
  group_by(subj, condition) %>% 
  summarise(mean = mean(count)) %>% 
  mutate(condition = factor(condition, 
                            levels = c("left", "none", "right")))
summary_pid
```

at the group level

```{r}
summary <- d1 %>% 
  group_by(condition) %>% 
  summarise(mean = mean(count),
            sd = sd(count),
            n = length(unique(subj)), # n here is the total subjs
            sem = (sd/sqrt(length(unique(subj)))),
            ci = sem*1.96) %>% 
  mutate(condition = factor(condition, 
                            levels = c("left", "none", "right")))
summary
```

violin

```{r}
p2.2 <- ggplot(summary_pid, aes(x=condition, y = mean, 
                                fill = condition, colour = condition)) +
   geom_jitter(position=position_jitterdodge(dodge.width =1), 
               alpha = 1, colour = "darkgrey") +
   geom_line(aes(group=subj), 
             colour = "darkgrey") +
   geom_violin(alpha = 0.7) +
   geom_point(data = summary, 
             aes(y = mean), size = 3, position=pd2, colour="black") +
   geom_errorbar(data = summary,
                aes(y = mean, ymin = mean-sem, ymax = mean +sem),
                width=.2, position=pd2, colour = "black") +
   geom_line(data = summary, 
            aes(y = mean, group=1),
            position=pd2,  colour = "black") +
   scale_fill_brewer(palette = "Dark2") +
   scale_colour_brewer(palette = "Dark2") +
   ggtitle("simdat1 count by stimulation condition")
p2.2

ggsave ("figures/sims/d1_count_violin.jpeg")
```

this also looks fairly sensible

## section 3 - build an initial model with one simulated dataset ##

This initial model will serve as the basis for other models to be built from using
the 'update' function in brms. It saves time this way.

The models will be built in brms.

## formula ##

```{r}
formula = bf(count ~ 1 + condition +
               (1 + condition | subj))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = d1, family = gaussian())
```

## visualise priors ##

here we would normally visualise priors of interest to make a judgment about what
would constitute weakly informative priors. 

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

intercept

```{r}
visualize("normal(14, 1)", "normal(14, 2)", "normal(14, 3)", "normal(14, 4)",
          xlim = c(5, 25))
```

effect of condition (b)

```{r}
visualize("normal(0, 1)", "normal(0, 2)", "normal(0, 3)", "normal(0, 4)",
          xlim = c(-6, 6))
```

## set priors ##

```{r}
priors <- c(
  set_prior("normal(14, 4)", class = "Intercept"),
  set_prior("normal(0, 4)", class = "b"),
  set_prior("normal(0, 2)", class = "sd"),
  set_prior("normal(0, 2)", class = "sigma"),
  set_prior("lkj(2)", class = "cor")
)
```

change the condition factor coding to make the labels simpler in the model output

```{r}
d1 <- d1 %>% 
  mutate(condition = factor(condition, 
                            levels = c("none", "left", "right")))
head(d1)
str(d1)
```

## fit the model ##

```{r}
t1 <- Sys.time()

plan(multicore)
fit <-
  brm(data = d1,
      family = gaussian(),
      formula = formula,
      prior = priors,
      iter = 2000, warmup = 1000, chains = 4,
      cores = 20, 
      save_pars = save_pars(all=TRUE),
      seed = 1)

t2 <- Sys.time()

t2 - t1

# Time difference 
# 25s macbook
# similar on iMac 
```

let's take a look 

```{r}
# chains
plot(fit)
# summary
print(fit)
# fixed effects
fixef(fit)

# save initial fit
saveRDS(fit, "models/sims/fit2_d1.rds") ## fit 2 is based on values derived from model b1, but with looser priors
```

## section 4 - update the fit ##

update the fit and check the time taken (to see the time difference when 
compared to not using the 'update' function).

```{r}
set.seed(2)

# create new data
d1u <- add_random(subj = subj_n, rep = rep_n) %>%
  # add and recode categorical variables
  add_within("subj", condition = c("none", "left", "right")) %>%
  add_contrast("condition", "treatment", add_cols = TRUE, 
               colnames = c("left", "right")) %>%
  # add random effects 
  add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd,
            .cors = cors) %>%
  add_ranef(sigma = sigma_sd) %>%
  # calculate dv
  mutate(count = b0 + u0s + (b1 + u1s) * left + (b2 + u2s) * right + sigma) %>% 
  mutate(condition = factor(condition, 
                            levels = c("none", "left", "right")))
head(d1u)

# update the fit and supply new data

t1 <- Sys.time()

plan(multicore)
updated_fit <-
  update(fit,
         newdata = d1u,
         seed = 2)

t2 <- Sys.time()

t2 - t1

# Time difference of 
# 4s macbook
# similar on the imac
```

let's take a look 

```{r}
# chains
plot(updated_fit)
# summary
print(updated_fit)
# fixed effects
fixef(updated_fit)

# save the updated fit
saveRDS(updated_fit, "models/sims/updated_fit_2_d1u.rds") # fit 2 is based on values derived from model b1, but with looser priors
```

## section 5 - build a simulation function ##

```{r}
sim1 <- function(subj_n = 12, rep_n = 8,  # these can be changed when calling the function
                b0 = 14.99, b1 = -1.71, b2 = -0.16,        # fixed effects
                u0s_sd = 1.14, u1s_sd = 1.39, u2s_sd = 0.4,   # varying effects
                cors = c(-0.52, -0.11,
                         0.34),   # cor
                sigma_sd = 1.26,           # error term
                ... # helps the function work with pmap() below
                ) {

  # set up data structure
  data <- add_random(subj = subj_n, rep = rep_n) %>%
  # add and recode categorical variables
  add_within("subj", condition = c("none", "left", "right")) %>%
  add_contrast("condition", "treatment", add_cols = TRUE, 
               colnames = c("left", "right")) %>%
  # add random effects 
  add_ranef("subj", u0s = u0s_sd, u1s = u1s_sd, u2s = u2s_sd,
            .cors = cors) %>%
  add_ranef(sigma = sigma_sd) %>%
  # calculate dv
  mutate(count = b0 + u0s + (b1 + u1s) * left + (b2 + u2s) * right + sigma) %>% 
  mutate(condition = factor(condition, 
                            levels = c("none", "left", "right")))

  # glimpse(data) # only use this when testing the code
}
```

test it

```{r}
# sim1(subj_n = 25, rep_n = 10)
```

# section 6 - run the full simulation ##

before running the full thing, I would always first check the pipeline with 2 reps.

## now run it with 1000 reps per variation ##

```{r}
plan(multicore)
x <- crossing(
  repx = 1:1000, # number of replicates
  subj_n = c(8, 12, 16), # range of subject N
  b1 = c(-1, -1.35, -1.71) # effect of the key interaction
) %>%
  mutate(d = pmap(., sim1)) %>%
  mutate(params = map2(d, repx, ~update(fit, newdata = .x, seed = .y) %>% # if you left the code here, then it would store the models and data
                     fixef() %>% 
                     data.frame() %>% 
                     rownames_to_column("parameter"))) %>% 
  select(-d) # adding this line in removes the data from the stored tibble 'x'. 
```

take a look

```{r}
head(x)
```

select parameters of interest to summarise and visualise

```{r}
parameters <-
  x %>% 
  unnest(params)
head(parameters)
```

save out parameters

```{r}
# save the parameters
write_csv(parameters, "data/sims/sim2_p1.csv") #  sim 2 is based on values derived from model b1, but with looser priors
```

alternatively, read in saved parameters, if already computed and saved.

```{r}
## read in
# parameters <- read_csv("data/sims/sim_p1.csv") 
# head(parameters)
```

rename parameters and create factors

```{r}
bmx_params <- parameters %>%
  filter(parameter != "Intercept") %>%
  mutate(repx = factor(repx),
         subj_n = factor(subj_n),
         b1 = factor(b1),
         parameter = if_else(parameter == "conditionleft", "left", "right"),
         parameter = factor(parameter, 
                            levels = c("left", "right")))
head(bmx_params)
str(bmx_params)
```

calculate average values

```{r}
bmx_params_qi <- bmx_params %>%
  filter(parameter == "left") %>% 
  group_by(subj_n, b1, parameter) %>% 
  median_qi(Estimate)
head(bmx_params_qi)
```

[note - these look sensible. Maybe a tiny pull towards zero based on the prior? 
But nothing to be bothered about, as they look very sensible, on average]

and plot

```{r}
p_bm_fixed <- ggplot(bmx_params, aes(x = Estimate, y = fct_rev(parameter), 
                                   fill=parameter)) +  
  geom_vline(xintercept = 0, color = "grey", alpha = 5/10) +
  stat_halfeye() +
  labs(title = "Avg. simulated coefficient plot for fixed effects (predictors)",
       x = NULL, y = NULL) +
  theme_bw() +
  scale_fill_brewer(palette = "Dark2") +
  theme(panel.grid   = element_blank(),
        panel.grid.major.y = element_line(color = alpha("firebrick4", 1/2), linetype = 3),
        axis.text.y  = element_text(hjust = 0),
        axis.ticks.y = element_blank(),
        legend.position = "none") +
  coord_cartesian(xlim =c(-2.5, 2.5)) +
  scale_x_continuous(breaks=seq(-2.5,2.5,0.5)) +
  facet_grid(fct_rev(subj_n)~fct_rev(b1))
p_bm_fixed

ggsave ("figures/sims/bm_p1_fixef.jpeg",
        width = 9, height = 7)
```

calculate power i.e., % Q97.5 < 0

```{r}
bm_power <- bmx_params %>% 
  filter(parameter == "left") %>%
  group_by(subj_n, b1) %>% # here we would group_by stuff that we varied in the sims
  mutate(check = ifelse(Q97.5 < 0, 1, 0)) %>% 
  summarise(power = mean(check))
bm_power
```

plot power

```{r}
p_bm_power <- ggplot(bm_power, aes(x = fct_rev(b1), y = subj_n, fill = power)) +
  geom_tile() +
  geom_text(aes(label = sprintf("%.3f", power)), color = "white", size = 10) +
  scale_fill_viridis_c(limits = c(0, 1)) +
  labs(x = "b1")
p_bm_power

ggsave ("figures/sims/bm_p1_power.jpeg")
```

plot parameters and include power as a text label

wrangle

```{r}
bm_plot_params <- bmx_params %>%
  filter(parameter == "left") %>%
  mutate(above_zero = if_else(Q97.5 > 0, "yes", "no"), 
         above_zero = factor(above_zero, levels = c("no", "yes"))) %>% 
  inner_join(bm_power, by = c("subj_n", "b1")) %>% 
  mutate(power = round(power * 100, 2)) 
head(bm_plot_params)
```

plot

```{r}
p_bm_params <- bm_plot_params %>%
  ggplot(aes(x = repx, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(fatten = 1/2, aes(colour=above_zero)) +
  geom_hline(yintercept = 0, colour = "red") +
  # geom_hline(aes(yintercept = 0.35), colour = "blue") + # this would add at the target effect size
  scale_colour_manual(values=c("darkgrey", "black")) +
  geom_text(aes(x=700, y=2,
                label = sprintf("%.1f%s", power, "% power")), 
            color = "darkgrey", size = 4) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "sim # (i.e., simulation index)",
       y = expression(beta("b1"))) +
  scale_x_discrete(breaks = c(1000)) +
  facet_grid(fct_rev(subj_n)~fct_rev(b1))
p_bm_params

ggsave ("figures/sims/bm_p1_parameters.jpeg",
        width = 8, height = 6)
```

save out some files

```{r}
write_csv(bmx_params, "data/sims/bmx_p1_params.csv")
write_csv(bmx_params_qi, "data/sims/bmx_p1_param_qi.csv")
```


## section 7 - we might evaluate precision by looking at the widths of the intervals ##

Instead of just ordering the point-ranges by their seed values, we might instead 
arrange them by the lower levels.

```{r}
# wrangle to order by Q2.5
plot_p <- bmx_params %>%
  filter(parameter == "left") %>% 
  arrange(subj_n, b1, Q2.5) %>%
  mutate(rank = rep(1:1000, times=9)) # 1000 models per variation
head(plot_p)

# plot
p_params <- plot_p %>%
  ggplot(aes(x = rank, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(fatten = 1/2) +
  geom_hline(aes(yintercept = 0), colour = "red") +
  # geom_hline(aes(yintercept = b1), colour = "blue") + # this would add a line at b1 - the target effect size
  # scale_colour_manual(values=c("darkgrey","black")) +
  # geom_text(aes(x=800, y=-1,
  #               label = sprintf("%.f%s", power, "% power")), color = "darkgrey", size = 4) +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_discrete("reordered by the lower level of the 95% intervals", breaks = NULL) +
  ylab(expression(beta[1])) +
  facet_grid(fct_rev(subj_n) ~ fct_rev(b1))
p_params

ggsave("figures/params_by_Q2.5.jpeg")
```

Notice how this arrangement highlights the differences in widths among the 
intervals. The wider the interval, the less precise the estimate. Some intervals
were wider than others, but all tended to hover in a similar range. We might 
quantify those ranges by computing a width variable.

```{r}
plot_p <-
  plot_p %>% 
  mutate(width = Q97.5 - Q2.5)

head(plot_p)
```

Here’s the width distribution.

```{r}
p_density <- plot_p %>% 
  ggplot(aes(x = width)) +
  geom_histogram(binwidth = .01) +
  geom_rug(linewidth = 1/6) +
  facet_grid(fct_rev(subj_n) ~ fct_rev(b1))
p_density

ggsave("figures/width_density.jpeg")
```

The widths of our 95% intervals range from 1-4 across the variation in 
parameters. 

N=12 or 16 looks pretty good, but this of course needs domain knowledge to 
interpret what level of precision might be useful.

Let’s focus a bit and take a random sample from a few of the simulation iterations.

Here, we just focus on one effect size and sample 10 simulations per sample size.

```{r}
set.seed(1)

plot_p %>%
  filter(b1 == "-1.35") %>% 
  group_by(subj_n, b1) %>% 
  sample_n(10) %>% 
  mutate(repx = repx %>% as.character()) %>%

  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = repx,
             colour = subj_n)) +
  # geom_vline(xintercept = c(0, .5), color = "white") +
  geom_pointrange() +
  labs(x = expression(beta[1]),
       y = "repx #") +
  # scale_y_continuous(breaks = c(25, 75, 25)) +
  # xlim(-1, 1) 
  facet_grid(fct_rev(subj_n)~fct_rev(b1), scales = "free_y")
```

So instead of focusing on rejecting a null hypothesis, we might instead 
determine the sample size we need to have most of our 95% 
intervals come in at a certain level of precision. This has been termed the 
accuracy in parameter estimation [AIPE; Maxwell et al. ( 2008); see also 
Kruschke ( 2015)] approach to sample size planning.

Thinking in terms of AIPE, in terms of precision, let’s say we wanted widths of 
3, 2. Here’s how we did with our sims.

```{r}
plot_p %>%
  group_by(subj_n, b1) %>% 
  mutate(below_3 = if_else(width < 3, 1, 0),
         below_2 = if_else(width < 2, 1, 0)) %>% 
  summarise(power_3 = mean(below_3),
            power_2 = mean(below_2))

#   subj_n b1    power_3 power_2
#   <fct>  <fct>   <dbl>   <dbl>
# 1 8      -1.71   0.895   0.295
# 2 8      -1.35   0.919   0.309
# 3 8      -1      0.927   0.32 
# 4 12     -1.71   0.999   0.734
# 5 12     -1.35   0.999   0.715
# 6 12     -1      1       0.741
# 7 16     -1.71   1       0.961
# 8 16     -1.35   1       0.96 
# 9 16     -1      1       0.972
```

ok, at N=16, widths < 1 still look good. 

Our simulation suggests we have about a >95% probability of achieving 95% CI 
widths of 1 or smaller with n=16.

That's not bad, I guess. But it's a little hard to tell. Still obsessed with 
clearing zero. We knew from the first sim that N=75 was good for clearing zero 
at b1=0.5 and above. So what has this told us? I guess it tells us where the 
limit is, in terms of precision, with a design like this?

Though a little nonsensical, the .8 criterion would give our AIPE analyses a 
sense of familiarity with traditional power analyses, which some reviewers might
appreciate. But in his text, Kruschke mentioned several other alternatives. 
One would be to set maximum value for our CI widths and simulate to find the nn 
necessary so all our simulations pass that criterion. Another would follow 
Joseph, Wolfson, and du Berger ( 1995, 1995), who suggested we shoot for an N 
that produces widths that pass that criterion on average. Here’s how we did 
based on the average-width criterion.

```{r}
plot_p %>%
  group_by(subj_n, b1) %>%
  summarise(avg_width = mean(width))

#  subj_n b1    avg_width
#   <fct>  <fct>     <dbl>
# 1 8      -1.71      2.28
# 2 8      -1.35      2.25
# 3 8      -1         2.25
# 4 12     -1.71      1.79
# 5 12     -1.35      1.81
# 6 12     -1         1.78
# 7 16     -1.71      1.54
# 8 16     -1.35      1.55
# 9 16     -1         1.53
```

ok, so the average for N=12 and N=16 are under 2. But all of these effect sizes
are a little arbitrary, so what the bother? I guess it helps guide the design 
of an experiment by informing which values are likely to be able to be separated
from which other values with what degree of precision and conistency over many
experiments. 
